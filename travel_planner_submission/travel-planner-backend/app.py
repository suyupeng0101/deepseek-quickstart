#!/usr/bin/env python3
# travel-planner backend - example
# Requirements: flask, requests
from flask import Flask, request, jsonify
import requests, os, json, time

app = Flask(__name__)

# load config (copy config.example.json -> config.json and edit if needed)
cfg_path = os.path.join(os.path.dirname(__file__), "config.json")
if os.path.exists(cfg_path):
    cfg = json.load(open(cfg_path, "r", encoding="utf-8"))
else:
    cfg = json.load(open(os.path.join(os.path.dirname(__file__), "config.example.json"), "r", encoding="utf-8"))

OLLAMA_API = cfg.get("OLLAMA_API", "http://localhost:11434")
OLLAMA_MODEL = cfg.get("OLLAMA_MODEL", "deepseek-v3")
MCP_12306 = cfg.get("MCP_12306_URL", "http://localhost:8080")
MCP_TIMEOUT = cfg.get("MCP_TIMEOUT", 30)


def query_12306(origin, destination, date_str):
    """
    Query the 12306-MCP server for trains between origin and destination on date_str.
    This function assumes the 12306 MCP server exposes a GET /get-tickets endpoint like:
      GET /get-tickets?from=ORIGIN&to=DEST&date=YYYY-MM-DD
    If the actual server uses different paths or payloads, update this function accordingly.
    """
    try:
        params = {"from": origin, "to": destination, "date": date_str}
        r = requests.get(f"{MCP_12306}/get-tickets", params=params, timeout=MCP_TIMEOUT)
        r.raise_for_status()
        return r.json()
    except Exception as e:
        return {"error": f"12306 query failed: {str(e)}"}


def call_deepseek(prompt, system=None, stream=False):
    """
    Call Ollama (DeepSeek) via REST API. Uses /api/chat if structured messages are provided.
    Simple usage: POST /api/generate with {model, prompt} or POST /api/chat with {model, messages}.
    We'll use /api/chat with a single user message for better chat semantics.
    """
    payload = {
        "model": OLLAMA_MODEL,
        "messages": [{"role": "user", "content": prompt}],
        "stream": False
    }
    try:
        r = requests.post(f"{OLLAMA_API}/api/chat", json=payload, timeout=60)
        r.raise_for_status()
        # Ollama returns streaming chunks by default; when stream=false final JSON has 'responses' or 'choices'
        data = r.json()
        # try common fields
        if isinstance(data, dict):
            # look for 'choices' or 'response' or 'outputs'
            if "choices" in data and len(data["choices"])>0:
                return data["choices"][0].get("message", {}).get("content") or data["choices"][0].get("content")
            if "response" in data:
                return data["response"]
            if "output" in data:
                return data["output"]
        return data
    except Exception as e:
        return {"error": f"deepseek call failed: {str(e)}"}


@app.route("/plan", methods=["POST"])
def plan():
    """
    POST /plan
    Payload JSON: { "origin": "Beijing", "destination": "Shanghai", "date": "2025-09-10", "preferences": "fastest, budget" }
    Response: JSON with 'plan' (generated by model) and 'tickets' (raw 12306 results)
    """
    body = request.get_json(force=True)
    origin = body.get("origin")
    destination = body.get("destination")
    date = body.get("date")
    prefs = body.get("preferences", "")

    if not origin or not destination or not date:
        return jsonify({"error": "origin, destination and date are required"}), 400

    # 1) query 12306 MCP server
    tickets = query_12306(origin, destination, date)

    # 2) compose prompt for DeepSeek
    tickets_summary = ""
    if isinstance(tickets, dict) and tickets.get("error"):
        tickets_summary = "无法获取 12306 列车数据：" + tickets.get("error")
    else:
        # create a short summary (first N trains) to keep the prompt compact
        try:
            trains = tickets.get("data") if isinstance(tickets, dict) else tickets
            if isinstance(trains, list):
                for t in trains[:8]:
                    # expect fields like train_no, depart_time, arrive_time, duration, seat_types, price
                    train_no = t.get("train_no") or t.get("train")
                    depart = t.get("depart_time") or t.get("start_time")
                    arrive = t.get("arrive_time") or t.get("end_time")
                    dur = t.get("duration") or t.get("time")
                    price = t.get("price") or t.get("prices") or ""
                    tickets_summary += f"- {train_no or 'N/A'} {depart or ''} -> {arrive or ''}  时长:{dur or ''}  票价:{price}\n"
            else:
                tickets_summary = json.dumps(trains)[:1000]
        except Exception:
            tickets_summary = str(tickets)[:1000]

    prompt = f\"\"\"你是一个旅行规划助理。用户计划从 {origin} 到 {destination} 出发，日期为 {date}。用户偏好：{prefs}。
以下是从 12306 MCP 服务获取的列车信息（节选）：
{tickets_summary}

请基于以上列车信息和用户偏好，给出一份旅行建议：
1. 推荐至少 2-3 种可行出行方案（例如：最快、最省钱、最舒适），并说明每种方案的优缺点和大致票价/时长。 
2. 给出行程注意事项（订票时间、换乘、到站/出发站注意事项）。
3. 如果需要，可给出后续 API 查询或过滤建议（例如只筛选高铁 G 字头）。
请以中文输出，结构清晰，便于直接展示在前端界面。
\"\"\"

    model_resp = call_deepseek(prompt)

    return jsonify({"plan": model_resp, "tickets": tickets})


@app.route("/health", methods=["GET"])
def health():
    return jsonify({"ok": True, "ollama": OLLAMA_API, "model": OLLAMA_MODEL, "12306_mcp": MCP_12306})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
