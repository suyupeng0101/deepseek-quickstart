{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e7c0c21",
   "metadata": {},
   "source": [
    "# QLoRA Fine-tuning with Unsloth on **FreedomIntelligence/medical-o1-reasoning-SFT**  \n",
    "**Base model:** `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B`  \n",
    "\n",
    "**Method:** QLoRA (4-bit) via **Unsloth**  \n",
    "\n",
    "**Goal:** Distill reasoning style and compare **pre** vs **post** generations.\n",
    "\n",
    "> Run this notebook on Google Colab / Kaggle (GPU T4/A100/L4). Internet must be enabled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43abc050",
   "metadata": {},
   "source": [
    "## 1) Environment setup & installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa67ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install \"unsloth>=2024.12.0\" \"torch>=2.3\" --index-url https://download.pytorch.org/whl/cu121 -U\n",
    "!pip -q install transformers accelerate peft trl datasets bitsandbytes evaluate rouge-score sacrebleu --upgrade\n",
    "!pip -q install \"xformers>=0.0.27\" --index-url https://download.pytorch.org/whl/cu121 -U || true\n",
    "import torch, sys, os, random\n",
    "print(\"Torch:\", torch.__version__, \"CUDA:\", torch.version.cuda, \"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a983a4ce",
   "metadata": {},
   "source": [
    "## 2) Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17fd2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    base_model: str = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "    dataset_name: str = \"FreedomIntelligence/medical-o1-reasoning-SFT\"\n",
    "    text_field_instruction: str = \"instruction\"\n",
    "    text_field_input: str = \"input\"\n",
    "    text_field_output: str = \"output\"\n",
    "    max_seq_len: int = 4096\n",
    "    micro_batch_size: int = 2\n",
    "    grad_accum_steps: int = 8\n",
    "    epochs: int = 1\n",
    "    lr: float = 2e-4\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.05\n",
    "    weight_decay: float = 0.0\n",
    "    warmup_ratio: float = 0.03\n",
    "    logging_steps: int = 10\n",
    "    eval_samples: int = 64\n",
    "    save_dir: str = \"outputs_deepseek_qwen15b_med_o1_qLoRA\"\n",
    "    max_new_tokens: int = 512\n",
    "    prompt_template_name: str = \"med_o1_reason\"\n",
    "CFG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafff040",
   "metadata": {},
   "source": [
    "## 3) Load dataset & build SFT samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2686888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random, json\n",
    "\n",
    "raw = load_dataset(CFG.dataset_name)\n",
    "print(raw)\n",
    "\n",
    "def build_prompt(inst, inp):\n",
    "    # Simple, deterministic instruction-following template\n",
    "    return f\"\"\"[INST] 你是一名严谨的医学辅助决策系统。请阅读“病人信息/主诉”，进行分步推理（可列出鉴别诊断），最后给出**非诊断性的**专业建议与就医检查建议。\n",
    "- 要求：\n",
    "  1) 列出关键线索\n",
    "  2) 分步分析（可能机制/风险点）\n",
    "  3) 可能诊断方向（Top-3，含不确定性）\n",
    "  4) 建议下一步检查/转诊科室\n",
    "  5) 安全提示（不能替代医生）\n",
    "病人信息/主诉：{inp}\n",
    "[/INST]\"\"\"\n",
    "\n",
    "def build_response(output):\n",
    "    # Use the provided high-quality reasoning + answer if available\n",
    "    return output\n",
    "\n",
    "def to_sft(example):\n",
    "    inst = example.get(CFG.text_field_instruction, \"\")\n",
    "    inp  = example.get(CFG.text_field_input, \"\")\n",
    "    out  = example.get(CFG.text_field_output, \"\")\n",
    "    return {\n",
    "        \"prompt\": build_prompt(inst, inp),\n",
    "        \"response\": build_response(out)\n",
    "    }\n",
    "\n",
    "train = raw[\"train\"].map(to_sft, remove_columns=raw[\"train\"].column_names)\n",
    "print(train[0])\n",
    "print(\"Train size:\", len(train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fc7f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    print(\"=\"*80)\n",
    "    print(train[i][\"prompt\"][:600])\n",
    "    print(\"---\")\n",
    "    print(train[i][\"response\"][:600])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89560f0",
   "metadata": {},
   "source": [
    "## 4) Load base model in 4-bit & prepare QLoRA (Unsloth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3976e7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_len = CFG.max_seq_len\n",
    "dtype = None  # let unsloth decide BF16/FP16\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = CFG.base_model,\n",
    "    max_seq_length = max_seq_len,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_training(\n",
    "    model,\n",
    "    lora_r = CFG.lora_r,\n",
    "    lora_alpha = CFG.lora_alpha,\n",
    "    lora_dropout = CFG.lora_dropout,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    target_modules = \"all-linear\",  # good default for Qwen-style\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6c73a7",
   "metadata": {},
   "source": [
    "## 5) Baseline generation (before fine-tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be6a96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from transformers import TextStreamer\n",
    "\n",
    "def generate_text(prompts, max_new_tokens=CFG.max_new_tokens, temp=0.7, top_p=0.9):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    for p in prompts:\n",
    "        toks = tokenizer(p, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            out_ids = model.generate(\n",
    "                **toks,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=temp,\n",
    "                top_p=top_p,\n",
    "                repetition_penalty=1.05,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        outputs.append(tokenizer.decode(out_ids[0], skip_special_tokens=True))\n",
    "    return outputs\n",
    "\n",
    "sample_idx = random.sample(range(len(train)), min(CFG.eval_samples, len(train)))\n",
    "eval_prompts = [train[i][\"prompt\"] for i in sample_idx]\n",
    "baseline_outputs = generate_text(eval_prompts, max_new_tokens=384)\n",
    "print(baseline_outputs[0][:1200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52619859",
   "metadata": {},
   "source": [
    "## 6) Tokenize SFT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bddb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "def format_example(ex):\n",
    "    # supervised fine-tuning pairs: prompt + response\n",
    "    text = ex[\"prompt\"] + \"\\n\" + ex[\"response\"]\n",
    "    return {\"text\": text}\n",
    "\n",
    "train_text = train.map(format_example, remove_columns=train.column_names)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tok = tokenizer  # alias\n",
    "def tokenize(examples):\n",
    "    toks = tok(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=CFG.max_seq_len)\n",
    "    toks[\"labels\"] = toks[\"input_ids\"].copy()\n",
    "    return toks\n",
    "\n",
    "tokenized = train_text.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "tokenized = tokenized.shuffle(seed=42)\n",
    "tokenized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f278438c",
   "metadata": {},
   "source": [
    "## 7) SFT Training (QLoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ad0686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "import math, os\n",
    "\n",
    "os.makedirs(CFG.save_dir, exist_ok=True)\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir = CFG.save_dir,\n",
    "    per_device_train_batch_size = CFG.micro_batch_size,\n",
    "    gradient_accumulation_steps = CFG.grad_accum_steps,\n",
    "    num_train_epochs = CFG.epochs,\n",
    "    learning_rate = CFG.lr,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    warmup_ratio = CFG.warmup_ratio,\n",
    "    logging_steps = CFG.logging_steps,\n",
    "    bf16 = torch.cuda.is_available(),\n",
    "    fp16 = (not torch.cuda.is_available()),\n",
    "    gradient_checkpointing = True,\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    weight_decay = CFG.weight_decay,\n",
    "    max_grad_norm = 0.3,\n",
    "    save_strategy = \"epoch\",\n",
    "    report_to = \"none\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = tokenized,\n",
    "    dataset_text_field = None,  # we already tokenized\n",
    "    args = train_args,\n",
    "    packing = False,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_state()\n",
    "# Save PEFT adapter\n",
    "trainer.model.save_pretrained(CFG.save_dir)\n",
    "tokenizer.save_pretrained(CFG.save_dir)\n",
    "print(\"Saved adapter to\", CFG.save_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8704111",
   "metadata": {},
   "source": [
    "## 8) Post-training evaluation & comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6e11fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model with adapter (safety)\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model, tok2 = FastLanguageModel.from_pretrained(\n",
    "    model_name = CFG.base_model,\n",
    "    max_seq_length = CFG.max_seq_len,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "base_model = PeftModel.from_pretrained(base_model, CFG.save_dir)\n",
    "base_model.eval()\n",
    "\n",
    "def generate_with(model, tokenizer, prompts, max_new_tokens=CFG.max_new_tokens):\n",
    "    outs = []\n",
    "    for p in prompts:\n",
    "        toks = tokenizer(p, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            out_ids = model.generate(\n",
    "                **toks,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.05,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        outs.append(tokenizer.decode(out_ids[0], skip_special_tokens=True))\n",
    "    return outs\n",
    "\n",
    "finetuned_outputs = generate_with(base_model, tok2, eval_prompts, max_new_tokens=384)\n",
    "\n",
    "print(\"=== SAMPLE BEFORE ===\")\n",
    "print(baseline_outputs[0][:1200])\n",
    "print(\"\\n=== SAMPLE AFTER ===\")\n",
    "print(finetuned_outputs[0][:1200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de7236a",
   "metadata": {},
   "source": [
    "## 9) Quantitative proxy metrics (ROUGE-L / BLEU vs dataset `response`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e70f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "refs = [train[i][\"response\"] for i in sample_idx]\n",
    "\n",
    "rouge_before = rouge.compute(predictions=baseline_outputs, references=refs, use_stemmer=True)\n",
    "rouge_after  = rouge.compute(predictions=finetuned_outputs, references=refs, use_stemmer=True)\n",
    "bleu_before  = bleu.compute(predictions=baseline_outputs, references=[[r] for r in refs])\n",
    "bleu_after   = bleu.compute(predictions=finetuned_outputs, references=[[r] for r in refs])\n",
    "\n",
    "def fmt(d): \n",
    "    return {k: round(float(v), 4) for k, v in d.items()}\n",
    "\n",
    "print(\"ROUGE-L (before):\", round(rouge_before[\"rougeL\"], 4), \" | (after):\", round(rouge_after[\"rougeL\"], 4))\n",
    "print(\"BLEU (before):\", round(bleu_before[\"score\"], 4), \" | (after):\", round(bleu_after[\"score\"], 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8c1346",
   "metadata": {},
   "source": [
    "## 10) Export qualitative comparison samples (Markdown table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd958c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "rows = []\n",
    "for i,(p,b,a,r) in enumerate(zip(eval_prompts, baseline_outputs, finetuned_outputs, refs)):\n",
    "    rows.append({\n",
    "        \"idx\": i,\n",
    "        \"prompt\": p[:2200],\n",
    "        \"baseline\": b[:2200],\n",
    "        \"finetuned\": a[:2200],\n",
    "        \"reference\": r[:2200]\n",
    "    })\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_markdown(\"comparison.md\", index=False)\n",
    "df.to_csv(\"comparison.csv\", index=False)\n",
    "print(\"Saved comparison.md and comparison.csv\")\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8f0f3f",
   "metadata": {},
   "source": [
    "## 11) Try your own prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b57fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"[INST] 请阅读下述主诉，按**关键线索→分步分析→可能诊断方向(Top-3)→建议检查与转诊→安全提示**的结构回答：\n",
    "主诉：男性，28岁，程序员，近一周熬夜后出现头晕、颈部疼痛、间歇性恶心。[/INST]\"\"\"\n",
    "print(generate_with(base_model, tok2, [user_prompt])[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fef4941",
   "metadata": {},
   "source": [
    "## 12) Notes & Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ad6710",
   "metadata": {},
   "source": [
    "- The adapter (LoRA weights) is saved in `outputs_deepseek_qwen15b_med_o1_qLoRA/`.  \n",
    "- Use `peft` to merge with base weights if needed for deployment.  \n",
    "- Files exported:\n",
    "  - `comparison.csv` and `comparison.md` for qualitative review.\n",
    "  - LoRA adapter directory with `adapter_config.json` and weight files."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
